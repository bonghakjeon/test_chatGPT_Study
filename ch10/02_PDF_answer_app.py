# TODO : ì•„ë˜ ì˜¤ë¥˜ ë©”ì‹œì§€ í•´ê²°í•˜ê¸° ìœ„í•´ ê°€ìƒí™˜ê²½ "ch10_env" -> í´ë” Lib -> í´ë” site-packages -> í´ë” googletrans -> client.py
#        -> def __init__ ìƒì„±ìì— ë“¤ì–´ê°€ëŠ” íŒŒë¼ë¯¸í„° "proxies" ìˆ˜ì • (2025.01.17 jbh)
# ì˜¤ë¥˜ ë©”ì‹œì§€ 
# "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. 
# This behaviour is the source of the following dependency conflicts.
# chromadb 0.6.3 requires httpx>=0.27.0, but you have httpx 0.13.3 which is incompatible.      
# langsmith 0.2.10 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible. 
# Successfully installed googletrans-3.1.0a0 h11-0.9.0 httpcore-0.9.1 httpx-0.13.3" 
# ì°¸ê³  URL - https://codemoney.tistory.com/entry/python-error-googletrans%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC-%ED%98%B8%ED%99%98%EB%AC%B8%EC%A0%9C-httpx%EC%B6%A9%EB%8F%8C
# ì°¸ê³  2 URL - https://stackoverflow.com/questions/72796594/attributeerror-module-httpcore-has-no-attribute-synchttptransport

# def __init__ ìƒì„±ìì— ë“¤ì–´ê°€ëŠ” íŒŒë¼ë¯¸í„° "proxies" ìˆ˜ì •
# (ìˆ˜ì • ì „) proxies: typing.Dict[str, httpcore.SyncHTTPTransport] = None
# (ìˆ˜ì • í›„) proxies: typing.Dict[str, httpcore.AsyncHTTPProxy] = None


##### ê¸°ë³¸ ì •ë³´ ì…ë ¥ #####
# Streamlit íŒ¨í‚¤ì§€ ì¶”ê°€
import streamlit as st
# PDF reader
from PyPDF2 import PdfReader
# Langchain íŒ¨í‚¤ì§€ë“¤
from langchain.chat_models import ChatOpenAI
# from langchain_openai import ChatOpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains.question_answering import load_qa_chain
# êµ¬ê¸€ ë²ˆì—­ API ë¶ˆëŸ¬ì˜¤ê¸°
# êµ¬ê¸€ ë²ˆì—­ê¸° íŒ¨í‚¤ì§€ í„°ë¯¸ë„ ì„¤ì¹˜ ëª…ë ¹ì–´
# pip install --no-deps googletrans==3.1.0a0
# ì°¸ê³  URL - https://zziii.tistory.com/entry/ERROR-pips-dependency-resolver-does-not-currently-take-into-account-all-the-packages-that-are-installed
from googletrans import Translator

##### ê¸°ëŠ¥ êµ¬í˜„ í•¨ìˆ˜ #####
# ì˜ì–´ë¡œ ëœ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ ë°›ê³  
# êµ¬ê¸€ ë²ˆì—­ê¸° API ê¸°ëŠ¥ì„ í™œìš©í•´ì„œ
# í•œê¸€ ë²ˆì—­ ê²°ê³¼ë¥¼ í™”ë©´ì— ì¶œë ¥í•´ì£¼ëŠ” í•¨ìˆ˜
def google_trans(messages):
    google = Translator()
    result = google.translate(messages, dest="ko")
    return result.text

##### ë©”ì¸ í•¨ìˆ˜ #####
def main():
    st.set_page_config(page_title="PDF analyzer", layout="wide")

    # ì‚¬ì´ë“œë°”
    with st.sidebar:

        # Open AI API í‚¤ ì…ë ¥ë°›ê¸°
        open_apikey = st.text_input(label='OPENAI API í‚¤', placeholder='Enter Your API Key', value='',type='password')
        
        # ì…ë ¥ë°›ì€ API í‚¤ í‘œì‹œ
        if open_apikey:
            st.session_state["OPENAI_API"] = open_apikey 
        st.markdown('---')
        
    # ë©”ì¸ê³µê°„
    st.header("PDF ë‚´ìš© ì§ˆë¬¸ í”„ë¡œê·¸ë¨ğŸ“œ")
    st.markdown('---')
    st.subheader("PDF íŒŒì¼ì„ ë„£ìœ¼ì„¸ìš”")
    # PDF íŒŒì¼ ë°›ê¸°
    pdf = st.file_uploader(" ", type="pdf")
    if pdf is not None:
        # PDF íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ê¸°
        pdf_reader = PdfReader(pdf)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()
        # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê¸°
        text_splitter = CharacterTextSplitter(
            separator="\n",
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        chunks = text_splitter.split_text(text)

        st.markdown('---')
        st.subheader("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
        # ì‚¬ìš©ì ì§ˆë¬¸ ë°›ê¸°
        user_question = st.text_input("Ask a question about your PDF:")
        if user_question:
            # ì„ë² ë”©/ ì‹œë©˜í‹± ì¸ë±ìŠ¤
            embeddings = OpenAIEmbeddings(openai_api_key=st.session_state["OPENAI_API"])
            knowledge_base = FAISS.from_texts(chunks, embeddings)
            
            docs = knowledge_base.similarity_search(user_question)

            # ì§ˆë¬¸í•˜ê¸°
            llm = ChatOpenAI(temperature=0,
                    openai_api_key=st.session_state["OPENAI_API"],
                    max_tokens=2000,
                    model_name='gpt-3.5-turbo',
                    request_timeout=120
                    )
            chain = load_qa_chain(llm, chain_type="stuff")
            response = chain.run(input_documents=docs, question=user_question)
            # ë‹µë³€ê²°ê³¼
            st.info(response)
            #í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê¸°
            if st.button(label="ë²ˆì—­í•˜ê¸°"):
                trans = google_trans(response)
                st.success(trans)

if __name__=='__main__':
    main()